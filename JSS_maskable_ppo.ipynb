{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c282c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting JSSEnv\n",
      "  Downloading JSSEnv-1.0.2-py3-none-any.whl.metadata (663 bytes)\n",
      "Collecting gym (from JSSEnv)\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "     ---------------------------------------- 0.0/721.7 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/721.7 kB ? eta -:--:--\n",
      "      --------------------------------------- 10.2/721.7 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/721.7 kB 220.2 kB/s eta 0:00:04\n",
      "     - ----------------------------------- 30.7/721.7 kB 220.2 kB/s eta 0:00:04\n",
      "     --- --------------------------------- 61.4/721.7 kB 328.2 kB/s eta 0:00:03\n",
      "     ------ ----------------------------- 133.1/721.7 kB 563.7 kB/s eta 0:00:02\n",
      "     ------------------ ------------------- 348.2/721.7 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 721.7/721.7 kB 2.4 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pandas (from JSSEnv)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy (from JSSEnv)\n",
      "  Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     --------------------------------- ------ 51.2/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting plotly (from JSSEnv)\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting imageio (from JSSEnv)\n",
      "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from JSSEnv) (7.0.0)\n",
      "Collecting requests (from JSSEnv)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting kaleido (from JSSEnv)\n",
      "  Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl.metadata (15 kB)\n",
      "Collecting pytest (from JSSEnv)\n",
      "  Downloading pytest-8.3.5-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting codecov (from JSSEnv)\n",
      "  Downloading codecov-2.1.13-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting coverage (from codecov->JSSEnv)\n",
      "  Downloading coverage-7.8.0-cp311-cp311-win_amd64.whl.metadata (8.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->JSSEnv)\n",
      "  Downloading charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->JSSEnv)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->JSSEnv)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->JSSEnv)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting cloudpickle>=1.2.0 (from gym->JSSEnv)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting gym_notices>=0.0.4 (from gym->JSSEnv)\n",
      "  Downloading gym_notices-0.0.8-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting pillow>=8.3.2 (from imageio->JSSEnv)\n",
      "  Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas->JSSEnv) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->JSSEnv)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->JSSEnv)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly->JSSEnv)\n",
      "  Downloading narwhals-1.35.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from plotly->JSSEnv) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pytest->JSSEnv) (0.4.6)\n",
      "Collecting iniconfig (from pytest->JSSEnv)\n",
      "  Downloading iniconfig-2.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pluggy<2,>=1.5 (from pytest->JSSEnv)\n",
      "  Downloading pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas->JSSEnv) (1.17.0)\n",
      "Downloading JSSEnv-1.0.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.7/7.9 MB 46.4 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.3/7.9 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.4/7.9 MB 21.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 4.1/7.9 MB 26.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.2/7.9 MB 25.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.2/7.9 MB 25.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.2/7.9 MB 21.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.3/7.9 MB 22.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 22.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading codecov-2.1.13-py2.py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/12.9 MB 51.2 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.3/12.9 MB 52.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.2/12.9 MB 44.4 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.2/12.9 MB 44.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.2/12.9 MB 23.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.9/12.9 MB 26.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.1/12.9 MB 26.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.1/12.9 MB 26.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.8/12.9 MB 20.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.5/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.9/12.9 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 15.9 MB/s eta 0:00:00\n",
      "Downloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
      "   ---------------------------------------- 0.0/315.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 315.8/315.8 kB 19.1 MB/s eta 0:00:00\n",
      "Downloading kaleido-0.2.1-py2.py3-none-win_amd64.whl (65.9 MB)\n",
      "   ---------------------------------------- 0.0/65.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.4/65.9 MB 29.2 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 1.7/65.9 MB 21.4 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 2.1/65.9 MB 22.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 2.6/65.9 MB 14.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 3.5/65.9 MB 15.9 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 3.8/65.9 MB 14.3 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 4.5/65.9 MB 14.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 6.3/65.9 MB 17.6 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 7.0/65.9 MB 17.2 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 8.0/65.9 MB 17.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 9.3/65.9 MB 19.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 10.5/65.9 MB 19.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 12.4/65.9 MB 24.2 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 14.1/65.9 MB 28.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 15.7/65.9 MB 31.2 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 15.7/65.9 MB 31.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 16.9/65.9 MB 29.8 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 17.8/65.9 MB 28.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 18.5/65.9 MB 27.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 19.1/65.9 MB 25.1 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 19.7/65.9 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 19.9/65.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 19.9/65.9 MB 21.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 20.4/65.9 MB 18.2 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 20.7/65.9 MB 17.7 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 20.8/65.9 MB 16.4 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 21.1/65.9 MB 15.2 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 22.7/65.9 MB 14.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 24.1/65.9 MB 14.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 25.3/65.9 MB 14.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 27.1/65.9 MB 15.6 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 28.8/65.9 MB 17.3 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 30.4/65.9 MB 24.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 31.5/65.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 31.5/65.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 31.5/65.9 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 32.6/65.9 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 34.5/65.9 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 34.6/65.9 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 34.6/65.9 MB 26.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 34.6/65.9 MB 26.2 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 35.3/65.9 MB 19.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 36.8/65.9 MB 19.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.5/65.9 MB 19.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.5/65.9 MB 19.9 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 37.5/65.9 MB 19.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 38.8/65.9 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 40.5/65.9 MB 16.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 41.8/65.9 MB 19.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 44.5/65.9 MB 20.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 47.1/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 49.7/65.9 MB 54.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 51.3/65.9 MB 54.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 53.0/65.9 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 54.0/65.9 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 54.9/65.9 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 56.9/65.9 MB 34.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 58.6/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 59.6/65.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 61.0/65.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 62.1/65.9 MB 29.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 63.6/65.9 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.2/65.9 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  65.9/65.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 65.9/65.9 MB 16.8 MB/s eta 0:00:00\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.6 MB 56.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 40.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.4/11.6 MB 35.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.9/11.6 MB 31.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.7/11.6 MB 28.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.3/11.6 MB 25.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.4/11.6 MB 25.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 26.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 26.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.2/11.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.6/11.6 MB 26.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 19.8 MB/s eta 0:00:00\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "   ---------------------------------------- 0.0/14.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.7/14.8 MB 44.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.9/14.8 MB 37.2 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.9/14.8 MB 37.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 4.6/14.8 MB 24.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 6.3/14.8 MB 28.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 7.6/14.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 9.3/14.8 MB 29.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 10.5/14.8 MB 31.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 12.1/14.8 MB 31.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.9/14.8 MB 38.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.8/14.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.8/14.8 MB 26.2 MB/s eta 0:00:00\n",
      "Downloading pytest-8.3.5-py3-none-any.whl (343 kB)\n",
      "   ---------------------------------------- 0.0/343.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 343.6/343.6 kB 20.8 MB/s eta 0:00:00\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 166.4/166.4 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "   ---------------------------------------- 0.0/102.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 102.4/102.4 kB 3.0 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "   ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 70.4/70.4 kB 3.8 MB/s eta 0:00:00\n",
      "Downloading narwhals-1.35.0-py3-none-any.whl (325 kB)\n",
      "   ---------------------------------------- 0.0/325.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 325.7/325.7 kB 21.0 MB/s eta 0:00:00\n",
      "Downloading pillow-11.2.1-cp311-cp311-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.8/2.7 MB 55.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.7/2.7 MB 34.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 24.2 MB/s eta 0:00:00\n",
      "Downloading pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 509.2/509.2 kB 16.1 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "   ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
      "   ------------------------------------- - 337.9/347.8 kB 20.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 347.8/347.8 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.7/128.7 kB ? eta 0:00:00\n",
      "Downloading coverage-7.8.0-cp311-cp311-win_amd64.whl (214 kB)\n",
      "   ---------------------------------------- 0.0/214.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 214.9/214.9 kB 6.4 MB/s eta 0:00:00\n",
      "Downloading iniconfig-2.1.0-py3-none-any.whl (6.0 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml): started\n",
      "  Building wheel for gym (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827740 sha256=14eb7428affa9214e8b68659fa92fe664b18c1fbd6e732743adca74a05ba7a53\n",
      "  Stored in directory: c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local\\pip\\cache\\wheels\\1c\\77\\9e\\9af5470201a0b0543937933ee99ba884cd237d2faefe8f4d37\n",
      "Successfully built gym\n",
      "Installing collected packages: pytz, kaleido, gym_notices, urllib3, tzdata, pluggy, pillow, numpy, narwhals, iniconfig, idna, coverage, cloudpickle, charset-normalizer, certifi, requests, pytest, plotly, pandas, imageio, gym, codecov, JSSEnv\n",
      "Successfully installed JSSEnv-1.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 cloudpickle-3.1.1 codecov-2.1.13 coverage-7.8.0 gym-0.26.2 gym_notices-0.0.8 idna-3.10 imageio-2.37.0 iniconfig-2.1.0 kaleido-0.2.1 narwhals-1.35.0 numpy-2.2.4 pandas-2.2.3 pillow-11.2.1 plotly-6.0.1 pluggy-1.5.0 pytest-8.3.5 pytz-2025.2 requests-2.32.3 tzdata-2025.2 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\nsiye\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install JSSEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2592a347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (2.2.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\nsiye\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymnasium) (4.13.2)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/965.4 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/965.4 kB 435.7 kB/s eta 0:00:03\n",
      "   -- ------------------------------------ 61.4/965.4 kB 550.5 kB/s eta 0:00:02\n",
      "   -------- ------------------------------- 194.6/965.4 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 450.6/965.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 965.4/965.4 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\nsiye\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd41ec7",
   "metadata": {},
   "source": [
    "Original Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727cfbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative reward: 653.6767676767678\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import plotly.figure_factory as ff\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class JssEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        \"\"\"\n",
    "        This environment model the job shop scheduling problem as a single agent problem:\n",
    "\n",
    "        -The actions correspond to a job allocation + one action for no allocation at this time step (NOPE action)\n",
    "\n",
    "        -We keep a time with next possible time steps\n",
    "\n",
    "        -Each time we allocate a job, the end of the job is added to the stack of time steps\n",
    "\n",
    "        -If we don't have a legal action (i.e. we can't allocate a job),\n",
    "        we automatically go to the next time step until we have a legal action\n",
    "\n",
    "        -\n",
    "        :param env_config: Ray dictionary of config parameter\n",
    "        \"\"\"\n",
    "        if env_config is None:\n",
    "            env_config = {\n",
    "                \"instance_path\": Path.home() / \"Downloads\" / \"instances\" / \"ta80\" #Path(__file__).parent.absolute() / \"instances\" / \"ta80\"\n",
    "            }\n",
    "        instance_path = env_config[\"instance_path\"]\n",
    "\n",
    "        # initial values for variables used for instance\n",
    "        self.jobs = 0\n",
    "        self.machines = 0\n",
    "        self.instance_matrix = None\n",
    "        self.jobs_length = None\n",
    "        self.max_time_op = 0\n",
    "        self.max_time_jobs = 0\n",
    "        self.nb_legal_actions = 0\n",
    "        self.nb_machine_legal = 0\n",
    "        # initial values for variables used for solving (to reinitialize when reset() is called)\n",
    "        self.solution = None\n",
    "        self.last_solution = None\n",
    "        self.last_time_step = float(\"inf\")\n",
    "        self.current_time_step = float(\"inf\")\n",
    "        self.next_time_step = list()\n",
    "        self.next_jobs = list()\n",
    "        self.legal_actions = None\n",
    "        self.time_until_available_machine = None\n",
    "        self.time_until_finish_current_op_jobs = None\n",
    "        self.todo_time_step_job = None\n",
    "        self.total_perform_op_time_jobs = None\n",
    "        self.needed_machine_jobs = None\n",
    "        self.total_idle_time_jobs = None\n",
    "        self.idle_time_jobs_last_op = None\n",
    "        self.state = None\n",
    "        self.illegal_actions = None\n",
    "        self.action_illegal_no_op = None\n",
    "        self.machine_legal = None\n",
    "        # initial values for variables used for representation\n",
    "        self.start_timestamp = datetime.datetime.now().timestamp()\n",
    "        self.sum_op = 0\n",
    "        with open(instance_path, \"r\") as instance_file:\n",
    "            for line_cnt, line_str in enumerate(instance_file, start=1):\n",
    "                split_data = list(map(int, line_str.split()))\n",
    "\n",
    "                if line_cnt == 1:\n",
    "                    self.jobs, self.machines = split_data\n",
    "                    self.instance_matrix = np.zeros((self.jobs, self.machines), dtype=(int, 2))\n",
    "                    self.jobs_length = np.zeros(self.jobs, dtype=int)\n",
    "                else:\n",
    "                    assert len(split_data) % 2 == 0 and len(split_data) // 2 == self.machines\n",
    "                    job_nb = line_cnt - 2\n",
    "                    for i in range(0, len(split_data), 2):\n",
    "                        machine, time = split_data[i], split_data[i + 1]\n",
    "                        self.instance_matrix[job_nb][i // 2] = (machine, time)\n",
    "                        self.max_time_op = max(self.max_time_op, time)\n",
    "                        self.jobs_length[job_nb] += time\n",
    "                        self.sum_op += time\n",
    "        self.max_time_jobs = max(self.jobs_length)\n",
    "        # check the parsed data are correct\n",
    "        assert self.max_time_op > 0\n",
    "        assert self.max_time_jobs > 0\n",
    "        assert self.jobs > 0\n",
    "        assert self.machines > 1, \"We need at least 2 machines\"\n",
    "        assert self.instance_matrix is not None\n",
    "        # allocate a job + one to wait\n",
    "        self.action_space = gym.spaces.Discrete(self.jobs + 1)\n",
    "        # used for plotting\n",
    "        self.colors = [\n",
    "            tuple([random.random() for _ in range(3)]) for _ in range(self.machines)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        matrix with the following attributes for each job:\n",
    "            -Legal job\n",
    "            -Left over time on the current op\n",
    "            -Current operation %\n",
    "            -Total left over time\n",
    "            -When next machine available\n",
    "            -Time since IDLE: 0 if not available, time otherwise\n",
    "            -Total IDLE time in the schedule\n",
    "        \"\"\"\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"action_mask\": gym.spaces.Box(0, 1, shape=(self.jobs + 1,)),\n",
    "                \"real_obs\": gym.spaces.Box(\n",
    "                    low=0.0, high=1.0, shape=(self.jobs, 7), dtype=float\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _get_current_state_representation(self):\n",
    "        self.state[:, 0] = self.legal_actions[:-1]\n",
    "        return {\n",
    "            \"real_obs\": self.state,\n",
    "            \"action_mask\": self.legal_actions,\n",
    "        }\n",
    "    \n",
    "    def get_legal_actions(self):\n",
    "        return self.legal_actions\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_time_step = 0\n",
    "        self.next_time_step = list()\n",
    "        self.next_jobs = list()\n",
    "        self.nb_legal_actions = self.jobs\n",
    "        self.nb_machine_legal = 0\n",
    "        # represent all the legal actions\n",
    "        self.legal_actions = np.ones(self.jobs + 1, dtype=bool)\n",
    "        self.legal_actions[self.jobs] = False\n",
    "        # used to represent the solution\n",
    "        self.solution = np.full((self.jobs, self.machines), -1, dtype=int)\n",
    "        self.time_until_available_machine = np.zeros(self.machines, dtype=int)\n",
    "        self.time_until_finish_current_op_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.todo_time_step_job = np.zeros(self.jobs, dtype=int)\n",
    "        self.total_perform_op_time_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.needed_machine_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.total_idle_time_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.idle_time_jobs_last_op = np.zeros(self.jobs, dtype=int)\n",
    "        self.illegal_actions = np.zeros((self.machines, self.jobs), dtype=bool)\n",
    "        self.action_illegal_no_op = np.zeros(self.jobs, dtype=bool)\n",
    "        self.machine_legal = np.zeros(self.machines, dtype=bool)\n",
    "        for job in range(self.jobs):\n",
    "            needed_machine = self.instance_matrix[job][0][0]\n",
    "            self.needed_machine_jobs[job] = needed_machine\n",
    "            if not self.machine_legal[needed_machine]:\n",
    "                self.machine_legal[needed_machine] = True\n",
    "                self.nb_machine_legal += 1\n",
    "        self.state = np.zeros((self.jobs, 7), dtype=float)\n",
    "        return self._get_current_state_representation()\n",
    "\n",
    "    def _prioritization_non_final(self):\n",
    "        if self.nb_machine_legal >= 1:\n",
    "            for machine in range(self.machines):\n",
    "                if self.machine_legal[machine]:\n",
    "                    final_job = list()\n",
    "                    non_final_job = list()\n",
    "                    min_non_final = float(\"inf\")\n",
    "                    for job in range(self.jobs):\n",
    "                        if (\n",
    "                            self.needed_machine_jobs[job] == machine\n",
    "                            and self.legal_actions[job]\n",
    "                        ):\n",
    "                            if self.todo_time_step_job[job] == (self.machines - 1):\n",
    "                                final_job.append(job)\n",
    "                            else:\n",
    "                                current_time_step_non_final = self.todo_time_step_job[\n",
    "                                    job\n",
    "                                ]\n",
    "                                time_needed_legal = self.instance_matrix[job][\n",
    "                                    current_time_step_non_final\n",
    "                                ][1]\n",
    "                                machine_needed_nextstep = self.instance_matrix[job][\n",
    "                                    current_time_step_non_final + 1\n",
    "                                ][0]\n",
    "                                if (\n",
    "                                    self.time_until_available_machine[\n",
    "                                        machine_needed_nextstep\n",
    "                                    ]\n",
    "                                    == 0\n",
    "                                ):\n",
    "                                    min_non_final = min(\n",
    "                                        min_non_final, time_needed_legal\n",
    "                                    )\n",
    "                                    non_final_job.append(job)\n",
    "                    if len(non_final_job) > 0:\n",
    "                        for job in final_job:\n",
    "                            current_time_step_final = self.todo_time_step_job[job]\n",
    "                            time_needed_legal = self.instance_matrix[job][\n",
    "                                current_time_step_final\n",
    "                            ][1]\n",
    "                            if time_needed_legal > min_non_final:\n",
    "                                self.legal_actions[job] = False\n",
    "                                self.nb_legal_actions -= 1\n",
    "\n",
    "    def _check_no_op(self):\n",
    "        self.legal_actions[self.jobs] = False\n",
    "        if (\n",
    "            len(self.next_time_step) > 0\n",
    "            and self.nb_machine_legal <= 3\n",
    "            and self.nb_legal_actions <= 4\n",
    "        ):\n",
    "            machine_next = set()\n",
    "            next_time_step = self.next_time_step[0]\n",
    "            max_horizon = self.current_time_step\n",
    "            max_horizon_machine = [\n",
    "                self.current_time_step + self.max_time_op for _ in range(self.machines)\n",
    "            ]\n",
    "            for job in range(self.jobs):\n",
    "                if self.legal_actions[job]:\n",
    "                    time_step = self.todo_time_step_job[job]\n",
    "                    machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                    time_needed = self.instance_matrix[job][time_step][1]\n",
    "                    end_job = self.current_time_step + time_needed\n",
    "                    if end_job < next_time_step:\n",
    "                        return\n",
    "                    max_horizon_machine[machine_needed] = min(\n",
    "                        max_horizon_machine[machine_needed], end_job\n",
    "                    )\n",
    "                    max_horizon = max(max_horizon, max_horizon_machine[machine_needed])\n",
    "            for job in range(self.jobs):\n",
    "                if not self.legal_actions[job]:\n",
    "                    if (\n",
    "                        self.time_until_finish_current_op_jobs[job] > 0\n",
    "                        and self.todo_time_step_job[job] + 1 < self.machines\n",
    "                    ):\n",
    "                        time_step = self.todo_time_step_job[job] + 1\n",
    "                        time_needed = (\n",
    "                            self.current_time_step\n",
    "                            + self.time_until_finish_current_op_jobs[job]\n",
    "                        )\n",
    "                        while (\n",
    "                            time_step < self.machines - 1 and max_horizon > time_needed\n",
    "                        ):\n",
    "                            machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                            if (\n",
    "                                max_horizon_machine[machine_needed] > time_needed\n",
    "                                and self.machine_legal[machine_needed]\n",
    "                            ):\n",
    "                                machine_next.add(machine_needed)\n",
    "                                if len(machine_next) == self.nb_machine_legal:\n",
    "                                    self.legal_actions[self.jobs] = True\n",
    "                                    return\n",
    "                            time_needed += self.instance_matrix[job][time_step][1]\n",
    "                            time_step += 1\n",
    "                    elif (\n",
    "                        not self.action_illegal_no_op[job]\n",
    "                        and self.todo_time_step_job[job] < self.machines\n",
    "                    ):\n",
    "                        time_step = self.todo_time_step_job[job]\n",
    "                        machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                        time_needed = (\n",
    "                            self.current_time_step\n",
    "                            + self.time_until_available_machine[machine_needed]\n",
    "                        )\n",
    "                        while (\n",
    "                            time_step < self.machines - 1 and max_horizon > time_needed\n",
    "                        ):\n",
    "                            machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                            if (\n",
    "                                max_horizon_machine[machine_needed] > time_needed\n",
    "                                and self.machine_legal[machine_needed]\n",
    "                            ):\n",
    "                                machine_next.add(machine_needed)\n",
    "                                if len(machine_next) == self.nb_machine_legal:\n",
    "                                    self.legal_actions[self.jobs] = True\n",
    "                                    return\n",
    "                            time_needed += self.instance_matrix[job][time_step][1]\n",
    "                            time_step += 1\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0.0\n",
    "        if action == self.jobs:\n",
    "            self.nb_machine_legal = 0\n",
    "            self.nb_legal_actions = 0\n",
    "            for job in range(self.jobs):\n",
    "                if self.legal_actions[job]:\n",
    "                    self.legal_actions[job] = False\n",
    "                    needed_machine = self.needed_machine_jobs[job]\n",
    "                    self.machine_legal[needed_machine] = False\n",
    "                    self.illegal_actions[needed_machine][job] = True\n",
    "                    self.action_illegal_no_op[job] = True\n",
    "            while self.nb_machine_legal == 0:\n",
    "                reward -= self.increase_time_step()\n",
    "            scaled_reward = self._reward_scaler(reward)\n",
    "            self._prioritization_non_final()\n",
    "            self._check_no_op()\n",
    "            return (\n",
    "                self._get_current_state_representation(),\n",
    "                scaled_reward,\n",
    "                self._is_done(),\n",
    "                {},\n",
    "            )\n",
    "        else:\n",
    "            current_time_step_job = self.todo_time_step_job[action]\n",
    "            machine_needed = self.needed_machine_jobs[action]\n",
    "            time_needed = self.instance_matrix[action][current_time_step_job][1]\n",
    "            reward += time_needed\n",
    "            self.time_until_available_machine[machine_needed] = time_needed\n",
    "            self.time_until_finish_current_op_jobs[action] = time_needed\n",
    "            self.state[action][1] = time_needed / self.max_time_op\n",
    "            to_add_time_step = self.current_time_step + time_needed\n",
    "            if to_add_time_step not in self.next_time_step:\n",
    "                index = bisect.bisect_left(self.next_time_step, to_add_time_step)\n",
    "                self.next_time_step.insert(index, to_add_time_step)\n",
    "                self.next_jobs.insert(index, action)\n",
    "            self.solution[action][current_time_step_job] = self.current_time_step\n",
    "            for job in range(self.jobs):\n",
    "                if (\n",
    "                    self.needed_machine_jobs[job] == machine_needed\n",
    "                    and self.legal_actions[job]\n",
    "                ):\n",
    "                    self.legal_actions[job] = False\n",
    "                    self.nb_legal_actions -= 1\n",
    "            self.nb_machine_legal -= 1\n",
    "            self.machine_legal[machine_needed] = False\n",
    "            for job in range(self.jobs):\n",
    "                if self.illegal_actions[machine_needed][job]:\n",
    "                    self.action_illegal_no_op[job] = False\n",
    "                    self.illegal_actions[machine_needed][job] = False\n",
    "            # if we can't allocate new job in the current timestep, we pass to the next one\n",
    "            while self.nb_machine_legal == 0 and len(self.next_time_step) > 0:\n",
    "                reward -= self.increase_time_step()\n",
    "            self._prioritization_non_final()\n",
    "            self._check_no_op()\n",
    "            # we then need to scale the reward\n",
    "            scaled_reward = self._reward_scaler(reward)\n",
    "            return (\n",
    "                self._get_current_state_representation(),\n",
    "                scaled_reward,\n",
    "                self._is_done(),\n",
    "                {},\n",
    "            )\n",
    "\n",
    "    def _reward_scaler(self, reward):\n",
    "        return reward / self.max_time_op\n",
    "\n",
    "    def increase_time_step(self):\n",
    "        \"\"\"\n",
    "        The heart of the logic his here, we need to increase every counter when we have a nope action called\n",
    "        and return the time elapsed\n",
    "        :return: time elapsed\n",
    "        \"\"\"\n",
    "        hole_planning = 0\n",
    "        next_time_step_to_pick = self.next_time_step.pop(0)\n",
    "        self.next_jobs.pop(0)\n",
    "        difference = next_time_step_to_pick - self.current_time_step\n",
    "        self.current_time_step = next_time_step_to_pick\n",
    "        for job in range(self.jobs):\n",
    "            was_left_time = self.time_until_finish_current_op_jobs[job]\n",
    "            if was_left_time > 0:\n",
    "                performed_op_job = min(difference, was_left_time)\n",
    "                self.time_until_finish_current_op_jobs[job] = max(\n",
    "                    0, self.time_until_finish_current_op_jobs[job] - difference\n",
    "                )\n",
    "                self.state[job][1] = (\n",
    "                    self.time_until_finish_current_op_jobs[job] / self.max_time_op\n",
    "                )\n",
    "                self.total_perform_op_time_jobs[job] += performed_op_job\n",
    "                self.state[job][3] = (\n",
    "                    self.total_perform_op_time_jobs[job] / self.max_time_jobs\n",
    "                )\n",
    "                if self.time_until_finish_current_op_jobs[job] == 0:\n",
    "                    self.total_idle_time_jobs[job] += difference - was_left_time\n",
    "                    self.state[job][6] = self.total_idle_time_jobs[job] / self.sum_op\n",
    "                    self.idle_time_jobs_last_op[job] = difference - was_left_time\n",
    "                    self.state[job][5] = self.idle_time_jobs_last_op[job] / self.sum_op\n",
    "                    self.todo_time_step_job[job] += 1\n",
    "                    self.state[job][2] = self.todo_time_step_job[job] / self.machines\n",
    "                    if self.todo_time_step_job[job] < self.machines:\n",
    "                        self.needed_machine_jobs[job] = self.instance_matrix[job][\n",
    "                            self.todo_time_step_job[job]\n",
    "                        ][0]\n",
    "                        self.state[job][4] = (\n",
    "                            max(\n",
    "                                0,\n",
    "                                self.time_until_available_machine[\n",
    "                                    self.needed_machine_jobs[job]\n",
    "                                ]\n",
    "                                - difference,\n",
    "                            )\n",
    "                            / self.max_time_op\n",
    "                        )\n",
    "                    else:\n",
    "                        self.needed_machine_jobs[job] = -1\n",
    "                        # this allow to have 1 is job is over (not 0 because, 0 strongly indicate that the job is a\n",
    "                        # good candidate)\n",
    "                        self.state[job][4] = 1.0\n",
    "                        if self.legal_actions[job]:\n",
    "                            self.legal_actions[job] = False\n",
    "                            self.nb_legal_actions -= 1\n",
    "            elif self.todo_time_step_job[job] < self.machines:\n",
    "                self.total_idle_time_jobs[job] += difference\n",
    "                self.idle_time_jobs_last_op[job] += difference\n",
    "                self.state[job][5] = self.idle_time_jobs_last_op[job] / self.sum_op\n",
    "                self.state[job][6] = self.total_idle_time_jobs[job] / self.sum_op\n",
    "        for machine in range(self.machines):\n",
    "            if self.time_until_available_machine[machine] < difference:\n",
    "                empty = difference - self.time_until_available_machine[machine]\n",
    "                hole_planning += empty\n",
    "            self.time_until_available_machine[machine] = max(\n",
    "                0, self.time_until_available_machine[machine] - difference\n",
    "            )\n",
    "            if self.time_until_available_machine[machine] == 0:\n",
    "                for job in range(self.jobs):\n",
    "                    if (\n",
    "                        self.needed_machine_jobs[job] == machine\n",
    "                        and not self.legal_actions[job]\n",
    "                        and not self.illegal_actions[machine][job]\n",
    "                    ):\n",
    "                        self.legal_actions[job] = True\n",
    "                        self.nb_legal_actions += 1\n",
    "                        if not self.machine_legal[machine]:\n",
    "                            self.machine_legal[machine] = True\n",
    "                            self.nb_machine_legal += 1\n",
    "        return hole_planning\n",
    "\n",
    "    def _is_done(self):\n",
    "        if self.nb_legal_actions == 0:\n",
    "            self.last_time_step = self.current_time_step\n",
    "            self.last_solution = self.solution\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        df = []\n",
    "        for job in range(self.jobs):\n",
    "            i = 0\n",
    "            while i < self.machines and self.solution[job][i] != -1:\n",
    "                dict_op = dict()\n",
    "                dict_op[\"Task\"] = \"Job {}\".format(job)\n",
    "                start_sec = self.start_timestamp + self.solution[job][i]\n",
    "                finish_sec = start_sec + self.instance_matrix[job][i][1]\n",
    "                dict_op[\"Start\"] = datetime.datetime.fromtimestamp(start_sec)\n",
    "                dict_op[\"Finish\"] = datetime.datetime.fromtimestamp(finish_sec)\n",
    "                dict_op[\"Resource\"] = \"Machine {}\".format(\n",
    "                    self.instance_matrix[job][i][0]\n",
    "                )\n",
    "                df.append(dict_op)\n",
    "                i += 1\n",
    "        fig = None\n",
    "        if len(df) > 0:\n",
    "            df = pd.DataFrame(df)\n",
    "            fig = ff.create_gantt(\n",
    "                df,\n",
    "                index_col=\"Resource\",\n",
    "                colors=self.colors,\n",
    "                show_colorbar=True,\n",
    "                group_tasks=True,\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                autorange=\"reversed\"\n",
    "            )  # otherwise tasks are listed from the bottom up\n",
    "        return fig\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = JssEnv()\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    while not done:\n",
    "        legal_actions = obs[\"action_mask\"]\n",
    "        actions = np.random.choice(\n",
    "            len(legal_actions), 1, p=(legal_actions / legal_actions.sum())\n",
    "        )[0]\n",
    "        obs, rewards, done, _ = env.step(actions)\n",
    "        cum_reward += rewards\n",
    "    print(f\"Cumulative reward: {cum_reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79092b7",
   "metadata": {},
   "source": [
    "Modified Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62daf3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import plotly.figure_factory as ff\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class JssEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "        \"\"\"\n",
    "        This environment model the job shop scheduling problem as a single agent problem:\n",
    "\n",
    "        -The actions correspond to a job allocation + one action for no allocation at this time step (NOPE action)\n",
    "\n",
    "        -We keep a time with next possible time steps\n",
    "\n",
    "        -Each time we allocate a job, the end of the job is added to the stack of time steps\n",
    "\n",
    "        -If we don't have a legal action (i.e. we can't allocate a job),\n",
    "        we automatically go to the next time step until we have a legal action\n",
    "\n",
    "        -\n",
    "        :param env_config: Ray dictionary of config parameter\n",
    "        \"\"\"\n",
    "        if env_config is None:\n",
    "            env_config = {\n",
    "                \"instance_path\": Path.home() / \"Downloads\" / \"instances\" / \"ta50\" #Path(__file__).parent.absolute() / \"instances\" / \"ta80\"\n",
    "            }\n",
    "        instance_path = env_config[\"instance_path\"]\n",
    "\n",
    "        # initial values for variables used for instance\n",
    "        self.jobs = 0\n",
    "        self.machines = 0\n",
    "        self.instance_matrix = None\n",
    "        self.jobs_length = None\n",
    "        self.max_time_op = 0\n",
    "        self.max_time_jobs = 0\n",
    "        self.nb_legal_actions = 0\n",
    "        self.nb_machine_legal = 0\n",
    "        # initial values for variables used for solving (to reinitialize when reset() is called)\n",
    "        self.solution = None\n",
    "        self.last_solution = None\n",
    "        self.last_time_step = float(\"inf\")\n",
    "        self.current_time_step = float(\"inf\")\n",
    "        self.next_time_step = list()\n",
    "        self.next_jobs = list()\n",
    "        self.legal_actions = None\n",
    "        self.time_until_available_machine = None\n",
    "        self.time_until_finish_current_op_jobs = None\n",
    "        self.todo_time_step_job = None\n",
    "        self.total_perform_op_time_jobs = None\n",
    "        self.needed_machine_jobs = None\n",
    "        self.total_idle_time_jobs = None\n",
    "        self.idle_time_jobs_last_op = None\n",
    "        self.state = None\n",
    "        self.illegal_actions = None\n",
    "        self.action_illegal_no_op = None\n",
    "        self.machine_legal = None\n",
    "        # initial values for variables used for representation\n",
    "        self.start_timestamp = datetime.datetime.now().timestamp()\n",
    "        self.sum_op = 0\n",
    "        with open(instance_path, \"r\") as instance_file:\n",
    "            for line_cnt, line_str in enumerate(instance_file, start=1):\n",
    "                split_data = list(map(int, line_str.split()))\n",
    "\n",
    "                if line_cnt == 1:\n",
    "                    self.jobs, self.machines = split_data\n",
    "                    self.instance_matrix = np.zeros((self.jobs, self.machines), dtype=(int, 2))\n",
    "                    self.jobs_length = np.zeros(self.jobs, dtype=int)\n",
    "                else:\n",
    "                    assert len(split_data) % 2 == 0 and len(split_data) // 2 == self.machines\n",
    "                    job_nb = line_cnt - 2\n",
    "                    for i in range(0, len(split_data), 2):\n",
    "                        machine, time = split_data[i], split_data[i + 1]\n",
    "                        self.instance_matrix[job_nb][i // 2] = (machine, time)\n",
    "                        self.max_time_op = max(self.max_time_op, time)\n",
    "                        self.jobs_length[job_nb] += time\n",
    "                        self.sum_op += time\n",
    "        self.max_time_jobs = max(self.jobs_length)\n",
    "        # check the parsed data are correct\n",
    "        assert self.max_time_op > 0\n",
    "        assert self.max_time_jobs > 0\n",
    "        assert self.jobs > 0\n",
    "        assert self.machines > 1, \"We need at least 2 machines\"\n",
    "        assert self.instance_matrix is not None\n",
    "        # allocate a job + one to wait\n",
    "        self.action_space = gym.spaces.Discrete(self.jobs + 1)\n",
    "        # used for plotting\n",
    "        self.colors = [\n",
    "            tuple([random.random() for _ in range(3)]) for _ in range(self.machines)\n",
    "        ]\n",
    "        \"\"\"\n",
    "        matrix with the following attributes for each job:\n",
    "            -Legal job\n",
    "            -Left over time on the current op\n",
    "            -Current operation %\n",
    "            -Total left over time\n",
    "            -When next machine available\n",
    "            -Time since IDLE: 0 if not available, time otherwise\n",
    "            -Total IDLE time in the schedule\n",
    "        \"\"\"\n",
    "        # self.observation_space = gym.spaces.Dict(\n",
    "        #     {\n",
    "        #         \"action_mask\": gym.spaces.Box(0, 1, shape=(self.jobs + 1,)),\n",
    "        #         \"real_obs\": gym.spaces.Box(\n",
    "        #             low=0.0, high=1.0, shape=(self.jobs, 7), dtype=float\n",
    "        #         ),\n",
    "        #     }\n",
    "        # )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                    low=0.0, high=1.0, shape=(self.jobs, 7), dtype=float\n",
    "                )\n",
    "    def _get_current_state_representation(self):\n",
    "        self.state[:, 0] = self.legal_actions[:-1]\n",
    "        # return {\n",
    "        #     \"real_obs\": self.state,\n",
    "        #     \"action_mask\": self.legal_actions,\n",
    "        # }\n",
    "        return self.state\n",
    "    \n",
    "    def get_action_mask(self):\n",
    "        return self.legal_actions.astype(int)\n",
    "    \n",
    "    def get_legal_actions(self):\n",
    "        return self.legal_actions\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.current_time_step = 0\n",
    "        self.next_time_step = list()\n",
    "        self.next_jobs = list()\n",
    "        self.nb_legal_actions = self.jobs\n",
    "        self.nb_machine_legal = 0\n",
    "        # represent all the legal actions\n",
    "        self.legal_actions = np.ones(self.jobs + 1, dtype=bool)\n",
    "        self.legal_actions[self.jobs] = False\n",
    "        # used to represent the solution\n",
    "        self.solution = np.full((self.jobs, self.machines), -1, dtype=int)\n",
    "        self.time_until_available_machine = np.zeros(self.machines, dtype=int)\n",
    "        self.time_until_finish_current_op_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.todo_time_step_job = np.zeros(self.jobs, dtype=int)\n",
    "        self.total_perform_op_time_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.needed_machine_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.total_idle_time_jobs = np.zeros(self.jobs, dtype=int)\n",
    "        self.idle_time_jobs_last_op = np.zeros(self.jobs, dtype=int)\n",
    "        self.illegal_actions = np.zeros((self.machines, self.jobs), dtype=bool)\n",
    "        self.action_illegal_no_op = np.zeros(self.jobs, dtype=bool)\n",
    "        self.machine_legal = np.zeros(self.machines, dtype=bool)\n",
    "        for job in range(self.jobs):\n",
    "            needed_machine = self.instance_matrix[job][0][0]\n",
    "            self.needed_machine_jobs[job] = needed_machine\n",
    "            if not self.machine_legal[needed_machine]:\n",
    "                self.machine_legal[needed_machine] = True\n",
    "                self.nb_machine_legal += 1\n",
    "        self.state = np.zeros((self.jobs, 7), dtype=float)\n",
    "        return self._get_current_state_representation(), {}\n",
    "\n",
    "    def _prioritization_non_final(self):\n",
    "        if self.nb_machine_legal >= 1:\n",
    "            for machine in range(self.machines):\n",
    "                if self.machine_legal[machine]:\n",
    "                    final_job = list()\n",
    "                    non_final_job = list()\n",
    "                    min_non_final = float(\"inf\")\n",
    "                    for job in range(self.jobs):\n",
    "                        if (\n",
    "                            self.needed_machine_jobs[job] == machine\n",
    "                            and self.legal_actions[job]\n",
    "                        ):\n",
    "                            if self.todo_time_step_job[job] == (self.machines - 1):\n",
    "                                final_job.append(job)\n",
    "                            else:\n",
    "                                current_time_step_non_final = self.todo_time_step_job[\n",
    "                                    job\n",
    "                                ]\n",
    "                                time_needed_legal = self.instance_matrix[job][\n",
    "                                    current_time_step_non_final\n",
    "                                ][1]\n",
    "                                machine_needed_nextstep = self.instance_matrix[job][\n",
    "                                    current_time_step_non_final + 1\n",
    "                                ][0]\n",
    "                                if (\n",
    "                                    self.time_until_available_machine[\n",
    "                                        machine_needed_nextstep\n",
    "                                    ]\n",
    "                                    == 0\n",
    "                                ):\n",
    "                                    min_non_final = min(\n",
    "                                        min_non_final, time_needed_legal\n",
    "                                    )\n",
    "                                    non_final_job.append(job)\n",
    "                    if len(non_final_job) > 0:\n",
    "                        for job in final_job:\n",
    "                            current_time_step_final = self.todo_time_step_job[job]\n",
    "                            time_needed_legal = self.instance_matrix[job][\n",
    "                                current_time_step_final\n",
    "                            ][1]\n",
    "                            if time_needed_legal > min_non_final:\n",
    "                                self.legal_actions[job] = False\n",
    "                                self.nb_legal_actions -= 1\n",
    "\n",
    "    def _check_no_op(self):\n",
    "        self.legal_actions[self.jobs] = False\n",
    "        if (\n",
    "            len(self.next_time_step) > 0\n",
    "            and self.nb_machine_legal <= 3\n",
    "            and self.nb_legal_actions <= 4\n",
    "        ):\n",
    "            machine_next = set()\n",
    "            next_time_step = self.next_time_step[0]\n",
    "            max_horizon = self.current_time_step\n",
    "            max_horizon_machine = [\n",
    "                self.current_time_step + self.max_time_op for _ in range(self.machines)\n",
    "            ]\n",
    "            for job in range(self.jobs):\n",
    "                if self.legal_actions[job]:\n",
    "                    time_step = self.todo_time_step_job[job]\n",
    "                    machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                    time_needed = self.instance_matrix[job][time_step][1]\n",
    "                    end_job = self.current_time_step + time_needed\n",
    "                    if end_job < next_time_step:\n",
    "                        return\n",
    "                    max_horizon_machine[machine_needed] = min(\n",
    "                        max_horizon_machine[machine_needed], end_job\n",
    "                    )\n",
    "                    max_horizon = max(max_horizon, max_horizon_machine[machine_needed])\n",
    "            for job in range(self.jobs):\n",
    "                if not self.legal_actions[job]:\n",
    "                    if (\n",
    "                        self.time_until_finish_current_op_jobs[job] > 0\n",
    "                        and self.todo_time_step_job[job] + 1 < self.machines\n",
    "                    ):\n",
    "                        time_step = self.todo_time_step_job[job] + 1\n",
    "                        time_needed = (\n",
    "                            self.current_time_step\n",
    "                            + self.time_until_finish_current_op_jobs[job]\n",
    "                        )\n",
    "                        while (\n",
    "                            time_step < self.machines - 1 and max_horizon > time_needed\n",
    "                        ):\n",
    "                            machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                            if (\n",
    "                                max_horizon_machine[machine_needed] > time_needed\n",
    "                                and self.machine_legal[machine_needed]\n",
    "                            ):\n",
    "                                machine_next.add(machine_needed)\n",
    "                                if len(machine_next) == self.nb_machine_legal:\n",
    "                                    self.legal_actions[self.jobs] = True\n",
    "                                    return\n",
    "                            time_needed += self.instance_matrix[job][time_step][1]\n",
    "                            time_step += 1\n",
    "                    elif (\n",
    "                        not self.action_illegal_no_op[job]\n",
    "                        and self.todo_time_step_job[job] < self.machines\n",
    "                    ):\n",
    "                        time_step = self.todo_time_step_job[job]\n",
    "                        machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                        time_needed = (\n",
    "                            self.current_time_step\n",
    "                            + self.time_until_available_machine[machine_needed]\n",
    "                        )\n",
    "                        while (\n",
    "                            time_step < self.machines - 1 and max_horizon > time_needed\n",
    "                        ):\n",
    "                            machine_needed = self.instance_matrix[job][time_step][0]\n",
    "                            if (\n",
    "                                max_horizon_machine[machine_needed] > time_needed\n",
    "                                and self.machine_legal[machine_needed]\n",
    "                            ):\n",
    "                                machine_next.add(machine_needed)\n",
    "                                if len(machine_next) == self.nb_machine_legal:\n",
    "                                    self.legal_actions[self.jobs] = True\n",
    "                                    return\n",
    "                            time_needed += self.instance_matrix[job][time_step][1]\n",
    "                            time_step += 1\n",
    "\n",
    "    def step(self, action: int):\n",
    "        reward = 0.0\n",
    "        if action == self.jobs:\n",
    "            self.nb_machine_legal = 0\n",
    "            self.nb_legal_actions = 0\n",
    "            for job in range(self.jobs):\n",
    "                if self.legal_actions[job]:\n",
    "                    self.legal_actions[job] = False\n",
    "                    needed_machine = self.needed_machine_jobs[job]\n",
    "                    self.machine_legal[needed_machine] = False\n",
    "                    self.illegal_actions[needed_machine][job] = True\n",
    "                    self.action_illegal_no_op[job] = True\n",
    "            while self.nb_machine_legal == 0:\n",
    "                reward -= self.increase_time_step()\n",
    "            scaled_reward = self._reward_scaler(reward)\n",
    "            self._prioritization_non_final()\n",
    "            self._check_no_op()\n",
    "            return (\n",
    "                self._get_current_state_representation(),\n",
    "                scaled_reward,\n",
    "                self._is_done(),\n",
    "                {}, {}\n",
    "            )\n",
    "        else:\n",
    "            current_time_step_job = self.todo_time_step_job[action]\n",
    "            machine_needed = self.needed_machine_jobs[action]\n",
    "            time_needed = self.instance_matrix[action][current_time_step_job][1]\n",
    "            reward += time_needed\n",
    "            self.time_until_available_machine[machine_needed] = time_needed\n",
    "            self.time_until_finish_current_op_jobs[action] = time_needed\n",
    "            self.state[action][1] = time_needed / self.max_time_op\n",
    "            to_add_time_step = self.current_time_step + time_needed\n",
    "            if to_add_time_step not in self.next_time_step:\n",
    "                index = bisect.bisect_left(self.next_time_step, to_add_time_step)\n",
    "                self.next_time_step.insert(index, to_add_time_step)\n",
    "                self.next_jobs.insert(index, action)\n",
    "            self.solution[action][current_time_step_job] = self.current_time_step\n",
    "            for job in range(self.jobs):\n",
    "                if (\n",
    "                    self.needed_machine_jobs[job] == machine_needed\n",
    "                    and self.legal_actions[job]\n",
    "                ):\n",
    "                    self.legal_actions[job] = False\n",
    "                    self.nb_legal_actions -= 1\n",
    "            self.nb_machine_legal -= 1\n",
    "            self.machine_legal[machine_needed] = False\n",
    "            for job in range(self.jobs):\n",
    "                if self.illegal_actions[machine_needed][job]:\n",
    "                    self.action_illegal_no_op[job] = False\n",
    "                    self.illegal_actions[machine_needed][job] = False\n",
    "            # if we can't allocate new job in the current timestep, we pass to the next one\n",
    "            while self.nb_machine_legal == 0 and len(self.next_time_step) > 0:\n",
    "                reward -= self.increase_time_step()\n",
    "            self._prioritization_non_final()\n",
    "            self._check_no_op()\n",
    "            # we then need to scale the reward\n",
    "            scaled_reward = self._reward_scaler(reward)\n",
    "            return (\n",
    "                self._get_current_state_representation(),\n",
    "                scaled_reward,\n",
    "                self._is_done(),\n",
    "                {}, {}\n",
    "            )\n",
    "\n",
    "    def _reward_scaler(self, reward):\n",
    "        return reward / self.max_time_op\n",
    "\n",
    "    def increase_time_step(self):\n",
    "        \"\"\"\n",
    "        The heart of the logic his here, we need to increase every counter when we have a nope action called\n",
    "        and return the time elapsed\n",
    "        :return: time elapsed\n",
    "        \"\"\"\n",
    "        hole_planning = 0\n",
    "        next_time_step_to_pick = self.next_time_step.pop(0)\n",
    "        self.next_jobs.pop(0)\n",
    "        difference = next_time_step_to_pick - self.current_time_step\n",
    "        self.current_time_step = next_time_step_to_pick\n",
    "        for job in range(self.jobs):\n",
    "            was_left_time = self.time_until_finish_current_op_jobs[job]\n",
    "            if was_left_time > 0:\n",
    "                performed_op_job = min(difference, was_left_time)\n",
    "                self.time_until_finish_current_op_jobs[job] = max(\n",
    "                    0, self.time_until_finish_current_op_jobs[job] - difference\n",
    "                )\n",
    "                self.state[job][1] = (\n",
    "                    self.time_until_finish_current_op_jobs[job] / self.max_time_op\n",
    "                )\n",
    "                self.total_perform_op_time_jobs[job] += performed_op_job\n",
    "                self.state[job][3] = (\n",
    "                    self.total_perform_op_time_jobs[job] / self.max_time_jobs\n",
    "                )\n",
    "                if self.time_until_finish_current_op_jobs[job] == 0:\n",
    "                    self.total_idle_time_jobs[job] += difference - was_left_time\n",
    "                    self.state[job][6] = self.total_idle_time_jobs[job] / self.sum_op\n",
    "                    self.idle_time_jobs_last_op[job] = difference - was_left_time\n",
    "                    self.state[job][5] = self.idle_time_jobs_last_op[job] / self.sum_op\n",
    "                    self.todo_time_step_job[job] += 1\n",
    "                    self.state[job][2] = self.todo_time_step_job[job] / self.machines\n",
    "                    if self.todo_time_step_job[job] < self.machines:\n",
    "                        self.needed_machine_jobs[job] = self.instance_matrix[job][\n",
    "                            self.todo_time_step_job[job]\n",
    "                        ][0]\n",
    "                        self.state[job][4] = (\n",
    "                            max(\n",
    "                                0,\n",
    "                                self.time_until_available_machine[\n",
    "                                    self.needed_machine_jobs[job]\n",
    "                                ]\n",
    "                                - difference,\n",
    "                            )\n",
    "                            / self.max_time_op\n",
    "                        )\n",
    "                    else:\n",
    "                        self.needed_machine_jobs[job] = -1\n",
    "                        # this allow to have 1 is job is over (not 0 because, 0 strongly indicate that the job is a\n",
    "                        # good candidate)\n",
    "                        self.state[job][4] = 1.0\n",
    "                        if self.legal_actions[job]:\n",
    "                            self.legal_actions[job] = False\n",
    "                            self.nb_legal_actions -= 1\n",
    "            elif self.todo_time_step_job[job] < self.machines:\n",
    "                self.total_idle_time_jobs[job] += difference\n",
    "                self.idle_time_jobs_last_op[job] += difference\n",
    "                self.state[job][5] = self.idle_time_jobs_last_op[job] / self.sum_op\n",
    "                self.state[job][6] = self.total_idle_time_jobs[job] / self.sum_op\n",
    "        for machine in range(self.machines):\n",
    "            if self.time_until_available_machine[machine] < difference:\n",
    "                empty = difference - self.time_until_available_machine[machine]\n",
    "                hole_planning += empty\n",
    "            self.time_until_available_machine[machine] = max(\n",
    "                0, self.time_until_available_machine[machine] - difference\n",
    "            )\n",
    "            if self.time_until_available_machine[machine] == 0:\n",
    "                for job in range(self.jobs):\n",
    "                    if (\n",
    "                        self.needed_machine_jobs[job] == machine\n",
    "                        and not self.legal_actions[job]\n",
    "                        and not self.illegal_actions[machine][job]\n",
    "                    ):\n",
    "                        self.legal_actions[job] = True\n",
    "                        self.nb_legal_actions += 1\n",
    "                        if not self.machine_legal[machine]:\n",
    "                            self.machine_legal[machine] = True\n",
    "                            self.nb_machine_legal += 1\n",
    "        return hole_planning\n",
    "\n",
    "    def _is_done(self):\n",
    "        if self.nb_legal_actions == 0:\n",
    "            self.last_time_step = self.current_time_step\n",
    "            self.last_solution = self.solution\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        df = []\n",
    "        for job in range(self.jobs):\n",
    "            i = 0\n",
    "            while i < self.machines and self.solution[job][i] != -1:\n",
    "                dict_op = dict()\n",
    "                dict_op[\"Task\"] = \"Job {}\".format(job)\n",
    "                start_sec = self.start_timestamp + self.solution[job][i]\n",
    "                finish_sec = start_sec + self.instance_matrix[job][i][1]\n",
    "                dict_op[\"Start\"] = datetime.datetime.fromtimestamp(start_sec)\n",
    "                dict_op[\"Finish\"] = datetime.datetime.fromtimestamp(finish_sec)\n",
    "                dict_op[\"Resource\"] = \"Machine {}\".format(\n",
    "                    self.instance_matrix[job][i][0]\n",
    "                )\n",
    "                df.append(dict_op)\n",
    "                i += 1\n",
    "        fig = None\n",
    "        if len(df) > 0:\n",
    "            df = pd.DataFrame(df)\n",
    "            fig = ff.create_gantt(\n",
    "                df,\n",
    "                index_col=\"Resource\",\n",
    "                colors=self.colors,\n",
    "                show_colorbar=True,\n",
    "                group_tasks=True,\n",
    "            )\n",
    "            fig.update_yaxes(\n",
    "                autorange=\"reversed\"\n",
    "            )  # otherwise tasks are listed from the bottom up\n",
    "        return fig\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     env = JssEnv()\n",
    "#     obs = env.reset()\n",
    "#     done = False\n",
    "#     cum_reward = 0\n",
    "#     while not done:\n",
    "#         legal_actions = obs[\"action_mask\"]\n",
    "#         actions = np.random.choice(\n",
    "#             len(legal_actions), 1, p=(legal_actions / legal_actions.sum())\n",
    "#         )[0]\n",
    "#         obs, rewards, done, _ = env.step(actions)\n",
    "#         cum_reward += rewards\n",
    "#     print(f\"Cumulative reward: {cum_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f34fc473",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = JssEnv()\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4c0240",
   "metadata": {},
   "source": [
    "Training PPO with action masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8424f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 696      |\n",
      "|    ep_rew_mean     | -27.3    |\n",
      "| time/              |          |\n",
      "|    fps             | 668      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 694         |\n",
      "|    ep_rew_mean          | -1.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004299206 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.762      |\n",
      "|    explained_variance   | 0.0153      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    value_loss           | 57.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 687          |\n",
      "|    ep_rew_mean          | 4.38         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 556          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088408645 |\n",
      "|    clip_fraction        | 0.0898       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.76        |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.42         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00956     |\n",
      "|    value_loss           | 26.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 677         |\n",
      "|    ep_rew_mean          | 19.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007571068 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.722      |\n",
      "|    explained_variance   | 0.665       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.52        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00936    |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 669          |\n",
      "|    ep_rew_mean          | 31.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090228375 |\n",
      "|    clip_fraction        | 0.0817       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.721       |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.17         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00977     |\n",
      "|    value_loss           | 21.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 663          |\n",
      "|    ep_rew_mean          | 40.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083501525 |\n",
      "|    clip_fraction        | 0.086        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.713       |\n",
      "|    explained_variance   | 0.909        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.12         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0152      |\n",
      "|    value_loss           | 5.95         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 660         |\n",
      "|    ep_rew_mean          | 40.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 521         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006419234 |\n",
      "|    clip_fraction        | 0.0645      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.9         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.84        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00895    |\n",
      "|    value_loss           | 8.56        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 654         |\n",
      "|    ep_rew_mean          | 40.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 520         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007018666 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.659      |\n",
      "|    explained_variance   | 0.924       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.08        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 651          |\n",
      "|    ep_rew_mean          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054782755 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.643       |\n",
      "|    explained_variance   | 0.86         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    value_loss           | 42.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 648         |\n",
      "|    ep_rew_mean          | 41.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006263123 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.47        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00786    |\n",
      "|    value_loss           | 10.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 646          |\n",
      "|    ep_rew_mean          | 39.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069994614 |\n",
      "|    clip_fraction        | 0.0722       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.626       |\n",
      "|    explained_variance   | 0.954        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.59         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00961     |\n",
      "|    value_loss           | 15.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 643         |\n",
      "|    ep_rew_mean          | 36          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006139272 |\n",
      "|    clip_fraction        | 0.0419      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.591      |\n",
      "|    explained_variance   | 0.95        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.8        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00722    |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 641         |\n",
      "|    ep_rew_mean          | 38.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004169969 |\n",
      "|    clip_fraction        | 0.0271      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.9        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0048     |\n",
      "|    value_loss           | 41.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 640         |\n",
      "|    ep_rew_mean          | 37.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007896846 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.577      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00757    |\n",
      "|    value_loss           | 27.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 638         |\n",
      "|    ep_rew_mean          | 37.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007527656 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.545      |\n",
      "|    explained_variance   | 0.973       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.36        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00957    |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 636         |\n",
      "|    ep_rew_mean          | 36.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 514         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007479579 |\n",
      "|    clip_fraction        | 0.0542      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.531      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.4        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    value_loss           | 16.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 635          |\n",
      "|    ep_rew_mean          | 37.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053137895 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.526       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20           |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    value_loss           | 28.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 633          |\n",
      "|    ep_rew_mean          | 38.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064191194 |\n",
      "|    clip_fraction        | 0.0591       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.501       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.5         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00795     |\n",
      "|    value_loss           | 14.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 632          |\n",
      "|    ep_rew_mean          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054345536 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.507       |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.99         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00484     |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 631         |\n",
      "|    ep_rew_mean          | 38.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005467184 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.493      |\n",
      "|    explained_variance   | 0.974       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.41        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00503    |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 630          |\n",
      "|    ep_rew_mean          | 39.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 512          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045366646 |\n",
      "|    clip_fraction        | 0.0382       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.499       |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.671        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00551     |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 629         |\n",
      "|    ep_rew_mean          | 39.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004599438 |\n",
      "|    clip_fraction        | 0.0477      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00569    |\n",
      "|    value_loss           | 17.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 629         |\n",
      "|    ep_rew_mean          | 39.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 510         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007318656 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 19          |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 628         |\n",
      "|    ep_rew_mean          | 38.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004139717 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.486      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.4        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 628          |\n",
      "|    ep_rew_mean          | 38.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043282714 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00586     |\n",
      "|    value_loss           | 25.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 627         |\n",
      "|    ep_rew_mean          | 38.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 508         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005706584 |\n",
      "|    clip_fraction        | 0.0423      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.462      |\n",
      "|    explained_variance   | 0.977       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.2         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00776    |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 627         |\n",
      "|    ep_rew_mean          | 38.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 507         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006247109 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.463      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.26        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    value_loss           | 7.95        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 627         |\n",
      "|    ep_rew_mean          | 36.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 507         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004533587 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.46       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.5         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0072     |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 626          |\n",
      "|    ep_rew_mean          | 37.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 507          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043777507 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.433       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.6         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.0047      |\n",
      "|    value_loss           | 12.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 626          |\n",
      "|    ep_rew_mean          | 38.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 120          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064242687 |\n",
      "|    clip_fraction        | 0.0666       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.43        |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.32         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00842     |\n",
      "|    value_loss           | 6.77         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 624          |\n",
      "|    ep_rew_mean          | 39.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 509          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056318166 |\n",
      "|    clip_fraction        | 0.0523       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.424       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00575     |\n",
      "|    value_loss           | 5.57         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 622         |\n",
      "|    ep_rew_mean          | 40.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 509         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004173745 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.397      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.79        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 3.69        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 619         |\n",
      "|    ep_rew_mean          | 42.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 510         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004535447 |\n",
      "|    clip_fraction        | 0.0466      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.81        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00383    |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 618          |\n",
      "|    ep_rew_mean          | 41.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 510          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066535207 |\n",
      "|    clip_fraction        | 0.065        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.414       |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.412        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00828     |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 616         |\n",
      "|    ep_rew_mean          | 40.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 140         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005868544 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.403      |\n",
      "|    explained_variance   | 0.969       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.44        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 11.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 616         |\n",
      "|    ep_rew_mean          | 38.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 511         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004477986 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.21        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00672    |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 615         |\n",
      "|    ep_rew_mean          | 39.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005252912 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.426      |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00791    |\n",
      "|    value_loss           | 4.82        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 615         |\n",
      "|    ep_rew_mean          | 39.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 512         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004808414 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.391      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.52        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00619    |\n",
      "|    value_loss           | 5.79        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 614          |\n",
      "|    ep_rew_mean          | 38.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 155          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064596613 |\n",
      "|    clip_fraction        | 0.0548       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.427       |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 4.72         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 614         |\n",
      "|    ep_rew_mean          | 38.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 513         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005128831 |\n",
      "|    clip_fraction        | 0.0505      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.383      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.06        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00749    |\n",
      "|    value_loss           | 7.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 613          |\n",
      "|    ep_rew_mean          | 40.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 513          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 163          |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063082916 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.873        |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    value_loss           | 3.17         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 613          |\n",
      "|    ep_rew_mean          | 42.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 167          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059716874 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.41        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.531        |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00837     |\n",
      "|    value_loss           | 2.49         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 613          |\n",
      "|    ep_rew_mean          | 42           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 171          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034766216 |\n",
      "|    clip_fraction        | 0.029        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.396       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.821        |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00397     |\n",
      "|    value_loss           | 4.05         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 612          |\n",
      "|    ep_rew_mean          | 43.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 514          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059017716 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.929        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00678     |\n",
      "|    value_loss           | 2.73         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 612         |\n",
      "|    ep_rew_mean          | 43.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004510233 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.41       |\n",
      "|    explained_variance   | 0.988       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.83        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 612          |\n",
      "|    ep_rew_mean          | 45.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 515          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047116564 |\n",
      "|    clip_fraction        | 0.0424       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.391       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.49         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00584     |\n",
      "|    value_loss           | 2.78         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 612         |\n",
      "|    ep_rew_mean          | 45.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 515         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005507149 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.58        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00642    |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 612         |\n",
      "|    ep_rew_mean          | 46.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 190         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005753887 |\n",
      "|    clip_fraction        | 0.0452      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.389      |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    value_loss           | 5.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 611         |\n",
      "|    ep_rew_mean          | 46.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 516         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005298467 |\n",
      "|    clip_fraction        | 0.057       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00725    |\n",
      "|    value_loss           | 2.76        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sb3_contrib.ppo_mask.ppo_mask.MaskablePPO at 0x1ea0334aed0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "def mask_fn(env: gym.Env) -> np.ndarray:\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.unwrapped.get_action_mask()\n",
    "\n",
    "\n",
    "\n",
    "env = ActionMasker(env, mask_fn)  # Wrap to enable masking\n",
    "env = Monitor(env)\n",
    "# MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "# with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "# retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "# a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "\n",
    "model = MaskablePPO(MaskableActorCriticPolicy, env,   verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "\n",
    "# Note that use of masks is manual and optional outside of learning,\n",
    "# so masking can be \"removed\" at testing time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cda3963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b297e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = env.reset()\n",
    "\n",
    "# Initialize control variables\n",
    "done = False\n",
    "\n",
    "# Run the simulation loop until completion\n",
    "while not done:\n",
    "    # Measure time taken to select an action\n",
    "    mask=env.unwrapped.get_action_mask()\n",
    "    action = model.predict(obs, action_masks=mask)\n",
    "\n",
    "    # Measure time taken to execute a step\n",
    "    obs, reward, done, info, _ = env.step(action[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
